{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1OevaErEpqakB3NVopdjwqwy5e-nwAfMe",
      "authorship_tag": "ABX9TyNt6/Dde72U/PAebyVWs2lh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1Proskuryakov1/VKR/blob/main/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "N1cb_Ykafpdx",
        "outputId": "e4b64d9c-93ad-42b3-e153-d19aa0b8b89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/__init__.py\", line 3, in <module>\n",
            "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 33, in <module>\n",
            "    import requests\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/__init__.py\", line 43, in <module>\n",
            "    import urllib3\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/__init__.py\", line 15, in <module>\n",
            "    from ._base_connection import _TYPE_BODY\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/_base_connection.py\", line 5, in <module>\n",
            "    from .util.connection import _TYPE_SOCKET_OPTIONS\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/__init__.py\", line 5, in <module>\n",
            "    from .request import SKIP_HEADER, SKIPPABLE_HEADERS, make_headers\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/request.py\", line 186, in <module>\n",
            "    class ChunksAndContentLength(typing.NamedTuple):\n",
            "  File \"/usr/lib/python3.11/typing.py\", line 2939, in __new__\n",
            "    setattr(nm_tpl, key, ns[key])\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "unzip:  cannot find or open news-category-dataset.zip, news-category-dataset.zip.zip or news-category-dataset.zip.ZIP.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "File /content/News_Category_Dataset_v3.json does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a900d1cf77d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/News_Category_Dataset_v3.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ujson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         ):\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {filepath_or_buffer} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File /content/News_Category_Dataset_v3.json does not exist"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json /content/\n",
        "\n",
        "# Создание папки для хранения Kaggle API ключа и перемещение kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# Загрузка датасета с Kaggle\n",
        "!kaggle datasets download -d rmisra/news-category-dataset\n",
        "\n",
        "# Распаковка загруженного датасета\n",
        "!unzip news-category-dataset.zip\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Импортируем библиотеки\n",
        "\n",
        "\n",
        "# Импортируем библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "path = '/content/News_Category_Dataset_v3.json'\n",
        "df = pd.read_json(path, lines=True)\n",
        "df.head()\n",
        "\n",
        "df.info()\n",
        "df.describe()\n",
        "df.isna().sum()\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        num_empty_descriptions = (df[column].apply(len) == 0).sum()\n",
        "        print(f\"Количество строк, где длина '{column}' равна нулю: {num_empty_descriptions}\")\n",
        "        empty_headlines = df[df['headline'].apply(len) == 0]\n",
        "        print(f\"Количество различных категорий: {df.category.nunique()}\\n\")\n",
        "print(f\"Категории: {df['category'].unique()}\\n\")\n",
        "print(\"Категория | Кол-во статей\")\n",
        "print(df['category'].value_counts())\n",
        "df.category = df.category.replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
        "              \"QUEER VOICES\": \"GROUPS VOICES\",\n",
        "              \"BUSINESS\": \"BUSINESS & FINANCES\",\n",
        "              \"PARENTS\": \"PARENTING\",\n",
        "              \"BLACK VOICES\": \"GROUPS VOICES\",\n",
        "              \"THE WORLDPOST\": \"WORLD NEWS\",\n",
        "              \"STYLE\": \"STYLE & BEAUTY\",\n",
        "              \"GREEN\": \"ENVIRONMENT\",\n",
        "              \"TASTE\": \"FOOD & DRINK\",\n",
        "              \"WORLDPOST\": \"WORLD NEWS\",\n",
        "              \"SCIENCE\": \"SCIENCE & TECH\",\n",
        "              \"TECH\": \"SCIENCE & TECH\",\n",
        "              \"MONEY\": \"BUSINESS & FINANCES\",\n",
        "              \"ARTS\": \"ARTS & CULTURE\",\n",
        "              \"COLLEGE\": \"EDUCATION\",\n",
        "              \"LATINO VOICES\": \"GROUPS VOICES\",\n",
        "              \"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
        "              \"FIFTY\": \"OTHER\",\n",
        "              \"GOOD NEWS\": \"OTHER\"}\n",
        "            )\n",
        "\n",
        "# График распределения категорий\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(y=df['category'], order=df['category'].value_counts().index)\n",
        "plt.title('Распределение категорий новостей')\n",
        "plt.xlabel('Количество статей')\n",
        "plt.ylabel('Категория')\n",
        "plt.show()\n",
        "\n",
        "# Вычисление длины заголовков\n",
        "headline_lengths = df['headline'].apply(len)\n",
        "\n",
        "# Вычисление квантилей\n",
        "quantiles = headline_lengths.quantile([0.25, 0.5, 0.75])\n",
        "\n",
        "# Построение гистограммы\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(headline_lengths, bins=50, kde=True)\n",
        "\n",
        "# Добавление квантилей на график\n",
        "for quantile in quantiles:\n",
        "    plt.axvline(quantile, color='r', linestyle='--')\n",
        "    # plt.text(quantile, plt.gca().get_ylim()[1]*0.9, f'{quantile:.2f}', color='r', ha='center')\n",
        "\n",
        "# Настройки графика\n",
        "plt.title('Распределение длины заголовков')\n",
        "plt.xlabel('Длина заголовка (кол-во символов)')\n",
        "plt.ylabel('Количество')\n",
        "plt.show()\n",
        "\n",
        "# Вычисление длины заголовков\n",
        "description_lengths = df['short_description'].apply(len)\n",
        "\n",
        "# Вычисление квантилей\n",
        "quantiles = description_lengths.quantile([0.25, 0.5, 0.75])\n",
        "\n",
        "# Построение гистограммы\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(description_lengths, bins=50, kde=True)\n",
        "\n",
        "# Добавление квантилей на график\n",
        "for quantile in quantiles:\n",
        "    plt.axvline(quantile, color='r', linestyle='--')\n",
        "    # plt.text(quantile, plt.gca().get_ylim()[1]*0.9, f'{quantile:.2f}', color='r', ha='center')\n",
        "\n",
        "# Настройки графика\n",
        "plt.title('Распределение длины описаний')\n",
        "plt.xlabel('Длина описания (кол-во символов)')\n",
        "plt.ylabel('Количество')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from matplotlib.cm import get_cmap\n",
        "\n",
        "# Преобразование даты в формат datetime и создание столбца с годом и месяцем\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year_month'] = df['date'].dt.to_period('M')\n",
        "\n",
        "categories = df['category'].unique()\n",
        "cmap = get_cmap('tab20', len(categories))\n",
        "\n",
        "plt.figure(figsize=(16,10))\n",
        "for i, category in enumerate(df['category'].unique()):\n",
        "    subset = df[df['category'] == category]\n",
        "    subset.groupby('year_month').size().plot(label=category, color=cmap(i))\n",
        "\n",
        "# Получение всех уникальных значений year_month\n",
        "all_year_months = df['year_month'].sort_values().unique()\n",
        "\n",
        "# x_labels = [all_year_months[0], all_year_months[len(all_year_months) // 2], all_year_months[-1]]\n",
        "num_labels = 10\n",
        "x_labels = [all_year_months[i] for i in range(0, len(all_year_months), len(all_year_months) // num_labels)]\n",
        "\n",
        "\n",
        "plt.title('Тренды новостей по категориям во времени', size=20)\n",
        "plt.xlabel('Дата')\n",
        "plt.ylabel('Количество новостей')\n",
        "plt.xticks(x_labels, [label.strftime('%Y-%m') for label in x_labels], rotation=45)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "categories = df['category'].unique()\n",
        "\n",
        "df['year_month'] = df['date'].dt.to_period('M').astype(str)\n",
        "\n",
        "figure, axis = plt.subplots(9, 3, figsize=(12, 15))\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    # Получение тренда для текущей категории\n",
        "    trend = df[df['category'] == category].groupby('year_month').size()\n",
        "\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "\n",
        "    # Построение графика для текущей категории\n",
        "    axis[row, col].plot(trend)\n",
        "    axis[row, col].set_title(category)\n",
        "\n",
        "    x_labels = [trend.index[0], trend.index[len(trend) // 2], trend.index[-1]]\n",
        "    axis[row, col].set_xticks(x_labels)\n",
        "    axis[row, col].set_xticklabels(x_labels)\n",
        "\n",
        "# Увеличение пространства между графиками\n",
        "plt.subplots_adjust(hspace=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Фильтрация данных для новостей до 2018 года\n",
        "df_before_2018 = df[df['date'] < '2018-01-01']\n",
        "\n",
        "# Создание сетки подграфиков\n",
        "figure, axis = plt.subplots(9, 3, figsize=(12, 15))\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    # Получение тренда для текущей категории и отфильтрованных данных\n",
        "    trend = df_before_2018[df_before_2018['category'] == category].groupby('year_month').size()\n",
        "\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "\n",
        "    # Построение графика для текущей категории\n",
        "    axis[row, col].plot(trend)\n",
        "    axis[row, col].set_title(category)\n",
        "    try:\n",
        "        x_labels = [trend.index[0], trend.index[len(trend) // 2], trend.index[-1]]\n",
        "        axis[row, col].set_xticks(x_labels)\n",
        "        axis[row, col].set_xticklabels(x_labels)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Увеличение пространства между графиками\n",
        "plt.subplots_adjust(hspace=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Фильтрация данных для новостей до 2018 года\n",
        "df_before_2018 = df[df['date'] < '2018-01-01']\n",
        "\n",
        "# Создание сетки подграфиков\n",
        "figure, axis = plt.subplots(9, 3, figsize=(12, 15))\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    # Получение тренда для текущей категории и отфильтрованных данных\n",
        "    trend = df_before_2018[df_before_2018['category'] == category].groupby('year_month').size()\n",
        "\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "\n",
        "    # Построение графика для текущей категории\n",
        "    axis[row, col].plot(trend)\n",
        "    axis[row, col].set_title(category)\n",
        "    try:\n",
        "        x_labels = [trend.index[0], trend.index[len(trend) // 2], trend.index[-1]]\n",
        "        axis[row, col].set_xticks(x_labels)\n",
        "        axis[row, col].set_xticklabels(x_labels)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Увеличение пространства между графиками\n",
        "plt.subplots_adjust(hspace=0.75)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Преобразование даты в формат datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Создание нового столбца с днем недели\n",
        "df['day_of_week'] = df['date'].dt.day_name()\n",
        "\n",
        "# Группировка данных по категориям и дням недели и подсчет количества публикаций\n",
        "grouped_data = df.groupby(['category', 'day_of_week']).size().unstack()\n",
        "\n",
        "# Создание графика для каждой категории\n",
        "fig, axes = plt.subplots(9, 3, figsize=(12, 15))\n",
        "\n",
        "for ax, (category, data) in zip(axes.flatten(), grouped_data.iterrows()):\n",
        "    data = data.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "    data.plot(kind='bar', ax=ax, color='skyblue')\n",
        "    ax.set_title(category)\n",
        "    ax.set_xticklabels(data.index.str[0], rotation=0)\n",
        "    ax.set_xlabel(None)\n",
        "\n",
        "# Увеличение пространства между графиками\n",
        "plt.subplots_adjust(wspace=0.25, hspace=0.75)\n",
        "\n",
        "plt.suptitle('Частота публикаций по дням недели для различных категорий', fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Подсчет количества статей для каждого автора\n",
        "author_counts = df['authors'].value_counts()\n",
        "\n",
        "# Определение топ-10 самых активных авторов\n",
        "top_10_authors = author_counts[1:11]\n",
        "\n",
        "# Построение столбчатой диаграммы для топ-10 самых активных авторов\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_10_authors.plot(kind='bar', color='skyblue')\n",
        "plt.title('Количество статей топ-10 самых активных авторов')\n",
        "plt.xlabel('Авторы')\n",
        "plt.ylabel('Количество статей')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "wc = WordCloud(max_words=1000,\n",
        "               min_font_size=10,\n",
        "               height=600,\n",
        "               width=1600,\n",
        "               background_color='black',\n",
        "               contour_color='black',\n",
        "               colormap='plasma',\n",
        "               repeat=False,\n",
        "               stopwords=STOPWORDS).generate(' '.join(df.headline))\n",
        "\n",
        "plt.title(\"Облако слов из всех заголовков\", size=15)\n",
        "plt.imshow(wc, interpolation= \"bilinear\")\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "wc = WordCloud(max_words=1000,\n",
        "               min_font_size=10,\n",
        "               height=600,\n",
        "               width=1600,\n",
        "               background_color='black',\n",
        "               contour_color='black',\n",
        "               colormap='plasma',\n",
        "               repeat=False,\n",
        "               stopwords=STOPWORDS).generate(' '.join(df.short_description))\n",
        "\n",
        "plt.title(\"Облако слов из всех описаний\", size=15)\n",
        "plt.imshow(wc, interpolation= \"bilinear\")\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(9, 3, figsize=(8, 16), subplot_kw=dict(xticks=[], yticks=[], frame_on=False))\n",
        "plt.subplots_adjust(hspace=0.35)\n",
        "for ax, category in zip(axes.flatten(), df['category'].unique()):\n",
        "    wordcloud = WordCloud(width=500, height=300, random_state=42, max_font_size=100, background_color='black',\n",
        "                         colormap='plasma', stopwords=STOPWORDS).generate(' '.join(df[df['category']==category]['headline']))\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.set_title(category)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(9, 3, figsize=(8, 15), subplot_kw=dict(xticks=[], yticks=[], frame_on=False))\n",
        "plt.subplots_adjust(hspace=0.35)\n",
        "for ax, category in zip(axes.flatten(), df['category'].unique()):\n",
        "    wordcloud = WordCloud(width=500, height=300, random_state=42, max_font_size=100, background_color='black',\n",
        "                         colormap='plasma', stopwords=STOPWORDS).generate(' '.join(df[df['category']==category]['short_description']))\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.set_title(category)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ngrams(texts, n, num_top_ngrams, category):\n",
        "    \"\"\"\n",
        "    Plot the top N n-grams for the given texts.\n",
        "\n",
        "    Args:\n",
        "        texts (Series): Texts to analyze.\n",
        "        n (int): The n in n-grams (e.g., 1 for unigrams, 2 for bigrams).\n",
        "        num_top_ngrams (int): Number of top n-grams to display.\n",
        "        category (str): Category of the texts.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    ngram_counts = X.sum(axis=0).A1\n",
        "\n",
        "    vocabulary = vectorizer.vocabulary_\n",
        "    ngrams = [gram for gram, idx in sorted(vocabulary.items(), key=lambda x: x[1])]\n",
        "\n",
        "    sorted_indices = ngram_counts.argsort()[::-1]\n",
        "    top_ngram_indices = sorted_indices[:num_top_ngrams]\n",
        "    top_ngrams = [ngrams[idx] for idx in top_ngram_indices]\n",
        "    top_ngram_counts = ngram_counts[top_ngram_indices]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(num_top_ngrams), top_ngram_counts, align='center', color='skyblue')\n",
        "    plt.yticks(range(num_top_ngrams), top_ngrams)\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('N-gram')\n",
        "    plt.title(f'Top {num_top_ngrams} {n}-grams - N-gram Analysis for {category} Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Пример использования для анализа топ 10 биграмм по категориям\n",
        "categories = df['category'].unique()\n",
        "for category in categories:\n",
        "    print(f\"N-gram Analysis for {category} Category:\")\n",
        "    category_texts = df[df['category'] == category]['headline']\n",
        "    plot_ngrams(category_texts, n=2, num_top_ngrams=10, category=category)\n",
        "    df['short_description'] = df['headline'] + df['short_description']\n",
        "    df = df[df['headline'] != '']\n",
        "df.info()\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Убедитесь, что у вас есть необходимые данные NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Perform various preprocessing steps on the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text.\n",
        "    \"\"\"\n",
        "    # Удаление специальных символов и знаков\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "   # Преобразование текста в строчные буквы\n",
        "    text = text.lower()\n",
        "\n",
        "    # Удаление  лишних пробелов\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Удаление неалфавитно-цифровых символов\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "    # Удаление слов, начинающиеся с хэш-тегов\n",
        "    text = re.sub(r'\\b#\\w+\\b', '', text)\n",
        "\n",
        "    # Удаление URL-адреса, начинающиеся с 'http'\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Лемматизация\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.split()\n",
        "    lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "    text = ' '.join(lemmas)\n",
        "\n",
        "   # Набор общих слов, которые нужно удалить\n",
        "    common_words = set(['news', 'article', 'report'])  # Add your common words here\n",
        "    text = ' '.join([word for word in text.split() if word not in common_words])\n",
        "\n",
        "    # Удаление цифровых символов\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Удаление слов, содержащих менее трех символовs\n",
        "    text = ' '.join([word for word in text.split() if len(word) >= 3])\n",
        "\n",
        "    return text\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"\n",
        "    Convert a Penn Treebank POS tag to a WordNet POS tag.\n",
        "\n",
        "    Args:\n",
        "        treebank_tag (str): The Penn Treebank POS tag.\n",
        "\n",
        "    Returns:\n",
        "        str: The corresponding WordNet POS tag.\n",
        "    \"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "def preprocess_text_2(text):\n",
        "    \"\"\"\n",
        "    Apply lemmatization to the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        str: The lemmatized text.\n",
        "    \"\"\"\n",
        "\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "    words = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "df = df[:5000]\n",
        "\n",
        "# Применение препроцессинга к столбцам 'headline' и 'short_description' в DataFrame\n",
        "df['headline'] = df['headline'].apply(preprocess_text)\n",
        "df['short_description'] = df['short_description'].apply(preprocess_text)\n",
        "\n",
        "# Применение лемматизации к столбцам 'headline' и 'short_description' и создание новых столбцов\n",
        "df['headline_lemmatized'] = df['headline'].apply(preprocess_text_2)\n",
        "df['short_description_lemmatized'] = df['short_description'].apply(preprocess_text_2)\n",
        "\n",
        "# Выведение на экран несколько первых строк DataFrame, чтобы убедиться, что предварительная обработка прошла как надо.\n",
        "print(df[['headline', 'headline_lemmatized', 'short_description', 'short_description_lemmatized']].head())\n",
        "df.sample(5)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ngrams(texts, n, num_top_ngrams, category):\n",
        "    \"\"\"\n",
        "    Plot the top N n-grams for the given texts.\n",
        "\n",
        "    Args:\n",
        "        texts (Series): Texts to analyze.\n",
        "        n (int): The n in n-grams (e.g., 1 for unigrams, 2 for bigrams).\n",
        "        num_top_ngrams (int): Number of top n-grams to display.\n",
        "        category (str): Category of the texts.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    ngram_counts = X.sum(axis=0).A1\n",
        "\n",
        "    vocabulary = vectorizer.vocabulary_\n",
        "    ngrams = [gram for gram, idx in sorted(vocabulary.items(), key=lambda x: x[1])]\n",
        "\n",
        "    sorted_indices = ngram_counts.argsort()[::-1]\n",
        "    top_ngram_indices = sorted_indices[:num_top_ngrams]\n",
        "    top_ngrams = [ngrams[idx] for idx in top_ngram_indices]\n",
        "    top_ngram_counts = ngram_counts[top_ngram_indices]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(num_top_ngrams), top_ngram_counts, align='center', color='skyblue')\n",
        "    plt.yticks(range(num_top_ngrams), top_ngrams)\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('N-gram')\n",
        "    plt.title(f'Top {num_top_ngrams} {n}-grams - N-gram Analysis for {category} Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Пример использования для анализа топ 10 биграмм по категориям\n",
        "categories = df['category'].unique()\n",
        "for category in categories:\n",
        "    print(f\"N-gram Analysis for {category} Category:\")\n",
        "    category_texts = df[df['category'] == category]['headline']\n",
        "    plot_ngrams(category_texts, n=2, num_top_ngrams=10, category=category)\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Предобработка текстов\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Удаление строк с пустыми заголовками или описаниями\n",
        "df = df[(df['headline'] != \"\") & (df['short_description'] != \"\")]\n",
        "\n",
        "# К описанию добавляем заголовок\n",
        "df['short_description'] = df['headline'] + \" \" + df['short_description']\n",
        "\n",
        "# Создание нового DataFrame для хранения выбранных записей\n",
        "sampled_df = pd.DataFrame()\n",
        "\n",
        "# Перебор всех уникальных категорий\n",
        "categories = df['category'].unique()\n",
        "\n",
        "for category in categories:\n",
        "    category_df = df[df['category'] == category]\n",
        "\n",
        "    # Если количество записей в категории меньше n, берем все записи, иначе берем n случайных записей\n",
        "    if len(category_df) < 500:\n",
        "        sampled_category_df = category_df\n",
        "    else:\n",
        "        sampled_category_df = category_df.sample(n=500, random_state=42)\n",
        "\n",
        "    # Добавляем выбранные записи в новый DataFrame\n",
        "    sampled_df = pd.concat([sampled_df, sampled_category_df], ignore_index=True)\n",
        "\n",
        "# Оставляем только нужные столбцы\n",
        "sampled_df = sampled_df[['short_description', 'category']]\n",
        "\n",
        "# Применение предобработки к столбцу short_description\n",
        "sampled_df['short_description'] = sampled_df['short_description'].apply(preprocess_text)\n",
        "\n",
        "# Энкодирование категорий\n",
        "label_encoder = LabelEncoder()\n",
        "sampled_df['category_encoded'] = label_encoder.fit_transform(sampled_df['category'])\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
        "    sampled_df['short_description'].tolist(), sampled_df['category_encoded'].tolist(),\n",
        "    test_size=0.2,  # 80-20 разделение\n",
        "    random_state=42  # Для воспроизводимости\n",
        ")\n",
        "\n",
        "# Создание кастомного датасета\n",
        "class NewsCategoryDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Создание датасетов и загрузчиков данных\n",
        "train_dataset = NewsCategoryDataset(train_texts, train_labels)\n",
        "eval_dataset = NewsCategoryDataset(eval_texts, eval_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Инициализация устройства\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Working on {}\".format(device))\n",
        "\n",
        "sampled_df.sample(5)\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "num_labels = len(sampled_df['category'].unique())\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Функция для оценки модели\n",
        "def evaluate_model(eval_loader, model, device, label_encoder):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in eval_loader:\n",
        "            inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = torch.tensor(labels).to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Используем все классы из label_encoder, даже если они отсутствуют в тестовых данных\n",
        "    cm = confusion_matrix(true_labels, predictions, labels=range(len(label_encoder.classes_)))\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "    report = classification_report(\n",
        "        true_labels,\n",
        "        predictions,\n",
        "        labels=range(len(label_encoder.classes_)),\n",
        "        target_names=label_encoder.classes_,\n",
        "        zero_division=0\n",
        "    )\n",
        "    print(report)\n",
        "\n",
        "# Обучение модели\n",
        "for epoch in range(7):  # Количество эпох обучения\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{7}\", leave=False)\n",
        "\n",
        "    for texts, labels in train_loader_tqdm:\n",
        "        # Токенизация текста\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Исправленное преобразование меток\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            labels = labels.detach().clone().to(device)\n",
        "        else:\n",
        "            labels = torch.tensor(labels, dtype=torch.long).to(device)  # Указание типа для стабильности\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Прямой проход\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Обратное распространение\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Обновление статистики\n",
        "        running_loss += loss.item()\n",
        "        train_loader_tqdm.set_postfix(loss=running_loss / (train_loader_tqdm.n + 1))  # Более точное среднее\n",
        "\n",
        "    # Вывод среднего лосса по эпохе\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")  # Форматирование вывода\n",
        "\n",
        "    # Оценка модели после каждой эпохи\n",
        "    evaluate_model(eval_loader, model, device, label_encoder)\n",
        "\n",
        "   # Сохранение обученной модели\n",
        "model_path = \"./saved_model\"\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "# Сохранение токенизатора\n",
        "tokenizer_path = \"./saved_tokenizer\"\n",
        "tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "# Для последующей загрузки\n",
        "model_path = \"/content/drive/MyDrive/bert_finetuned_model/model\"\n",
        "tokenizer_path = \"/content/drive/MyDrive/bert_finetuned_model/tokenizer\"\n",
        "\n",
        "# Загрузка сохранённой модели\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Загрузка сохранённого токенизатора\n",
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Оценка модели на тестовой выборке\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in eval_loader:\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "# Вычисление точности\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Матрица ошибок\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Отчёт о классификации\n",
        "report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
        "print(report)\n",
        "\n",
        "\n",
        "# Создание нового DataFrame для хранения выбранных записей\n",
        "user_df = pd.DataFrame()\n",
        "\n",
        "# Перебор всех уникальных категорий\n",
        "categories = df['category'].unique()\n",
        "\n",
        "for category in categories:\n",
        "    category_df = df[df['category'] == category]\n",
        "\n",
        "    # Если количество записей в категории меньше n, берем все записи, иначе берем n случайных записей\n",
        "    if len(category_df) < 250:\n",
        "        sampled_category_df = category_df\n",
        "    else:\n",
        "        sampled_category_df = category_df.sample(n=250)\n",
        "\n",
        "    # Добавляем выбранные записи в новый DataFrame\n",
        "    user_df = pd.concat([user_df, sampled_category_df], ignore_index=True)\n",
        "\n",
        "# user_df = df[df['category'] == \"POLITICS\"].sample(n=100, random_state=42)\n",
        "\n",
        "# Оставляем только нужные столбцы\n",
        "user_df = user_df[['short_description', 'category']]\n",
        "\n",
        "# Применение предобработки к столбцу short_description\n",
        "user_df['short_description'] = user_df['short_description'].apply(preprocess_text)\n",
        "\n",
        "# Энкодирование категорий\n",
        "label_encoder = LabelEncoder()\n",
        "user_df['category_encoded'] = label_encoder.fit_transform(user_df['category'])\n",
        "user_df.sample(5)\n",
        "\n",
        "# Создание кастомного датасета\n",
        "user_texts = user_df['short_description'].tolist()\n",
        "user_labels = user_df['category_encoded'].tolist()\n",
        "\n",
        "user_dataset = NewsCategoryDataset(user_texts, user_labels)\n",
        "user_loader = DataLoader(user_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "# Оценка модели на пользовательской выборке\n",
        "model.eval()\n",
        "user_predictions = []\n",
        "user_true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in user_loader:\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        user_predictions.extend(predicted.cpu().numpy())\n",
        "        user_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Вычисление точности\n",
        "user_accuracy = accuracy_score(user_true_labels, user_predictions)\n",
        "print(f\"Accuracy on user set: {user_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Матрица ошибок\n",
        "user_cm = confusion_matrix(user_true_labels, user_predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(user_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Отчёт о классификации\n",
        "user_report = classification_report(user_true_labels, user_predictions, target_names=label_encoder.classes_)\n",
        "print(user_report)\n",
        "\n",
        "\n",
        "# Функция для предсказания категории для нового текста\n",
        "def predict_category(text):\n",
        "    text = preprocess_text(text)\n",
        "    print(f\"Preprocessed text: {text}\")\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "    predicted_category = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
        "    return predicted_category[0]\n",
        "\n",
        "# Пример использования функции предсказания\n",
        "example_text = \"yummy sandwiches and cola are perfect for a lunch in sunny new york\"\n",
        "example = df.sample()\n",
        "# Извлекаем значение столбца \"short_description\" для этого примера\n",
        "# example_text = example['short_description'].values[0]\n",
        "print(\"Example text:\", example_text)\n",
        "predicted_category = predict_category(example_text)\n",
        "# real_category = example['category'].values[0]\n",
        "# print(f\"Real category: {real_category}\")\n",
        "print(f\"Predicted category: {predicted_category}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xA1-PDGVkvqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbJi00EGplxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "FGg8PLP_jUTs"
      }
    }
  ]
}